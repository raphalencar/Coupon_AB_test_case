{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNihD35+cxGrrmYJE40L1Wg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raphalencar/Coupon_AB_test_case/blob/main/Load_raw_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf \"/content/download_dir/\"\n",
        "# !rm -rf \"/content/raw_parquet/\"\n",
        "# !rm -rf \"/content/sample_data/\""
      ],
      "metadata": {
        "id": "RdyABDgJV6B7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark install"
      ],
      "metadata": {
        "id": "J0qsCrFmXDbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "KOrbitmeXAqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0a9a8c-0f66-4af2-b91a-d428326538f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "WzvGkcs2XL_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RXqqYX_fWsqo"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "import os\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "vy7ZYx22eNXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(url, local_path):\n",
        "    file_name = os.path.basename(local_path)\n",
        "    # local_file_path = os.path.join(local_path, file_name)\n",
        "\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"'{file_name}' already exists.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Loading '{file_name}'...\")\n",
        "        # Adiciona -nc para não sobrescrever e remove -q para mostrar progresso\n",
        "        # O -O garante que o arquivo seja salvo com o nome especificado em local_path\n",
        "        !wget -nc $url -O {local_path}\n",
        "\n",
        "        if not (os.path.exists(local_path) and os.path.getsize(local_path) > 0):\n",
        "            print(f\"Download '{file_name}' failed or is empty.\")\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "def read_data(source_path, spark_read_func, parquet_output_path, timestamp_partition_cols=None, **kwargs):\n",
        "    file_name = os.path.basename(source_path)\n",
        "    df_raw = None\n",
        "\n",
        "    if os.path.exists(parquet_output_path) and len(os.listdir(parquet_output_path)) > 0:\n",
        "        print(f\"Reading '{file_name.replace('.', '_')}_df' from existing Parquet: {parquet_output_path}\")\n",
        "        df_raw = spark.read.parquet(parquet_output_path)\n",
        "    else:\n",
        "        df_raw = save_to_parquet(df_raw, source_path, file_name, spark_read_func, parquet_output_path, timestamp_partition_cols, **kwargs)\n",
        "\n",
        "    if df_raw:\n",
        "        print(f\"\\nDataFrame '{file_name.replace('.', '_')}_df' from Parquet (raw):\")\n",
        "        df_raw.printSchema()\n",
        "        df_raw.show(5)\n",
        "    return df_raw\n",
        "\n",
        "def save_to_parquet(df, source_path, file_name, spark_read_func, parquet_output_path, timestamp_partition_cols, **kwargs):\n",
        "    print(f\"Reading '{file_name.replace('.', '_')}_df' from raw source: {source_path}\")\n",
        "    try:\n",
        "        # salvando no formato parquet e particionando para melhor performance\n",
        "        df = spark_read_func(source_path, **kwargs)\n",
        "\n",
        "        prepare_df_for_partitioning(df, timestamp_partition_cols)\n",
        "\n",
        "        # df = cast_to_timestamp(df, timestamp_partition_cols)\n",
        "        print(f\"Initial load of '{file_name.replace('.', '_')}_df' from raw source completed.\")\n",
        "\n",
        "        print(f\"Saving '{file_name.replace('.', '_')}_df' to raw Parquet: {parquet_output_path}\")\n",
        "        df_writer = df.write.mode(\"overwrite\")\n",
        "        if timestamp_partition_cols:\n",
        "            print(f\"Partitioning raw data by: {timestamp_partition_cols}\")\n",
        "            df_writer = df_writer.partitionBy(*timestamp_partition_cols)\n",
        "\n",
        "        df_writer.parquet(parquet_output_path)\n",
        "        print(f\"Successfully saved '{file_name.replace('.', '_')}_df' to raw Parquet.\")\n",
        "\n",
        "        return spark.read.parquet(parquet_output_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading '{file_name}' and saving to Parquet: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_df_for_partitioning(df_input, date_col_name, partition_cols_names):\n",
        "    if date_col_name in df_input.columns:\n",
        "        print(f\"Preparing for raw partitioning: converting '{date_col_name}' and extracting date components for partitioning.\")\n",
        "\n",
        "        # Converte para Timestamp primeiro, se ainda não for\n",
        "        # Se a coluna já é Timestamp/Date, usa ela diretamente para extração\n",
        "        if df_input.schema[date_col_name].dataType not in (TimestampType(), DateType()):\n",
        "            df_input = df_input.withColumn(f\"{date_col_name}_temp_ts\", to_timestamp(col(date_col_name)))\n",
        "            col_to_use_for_extraction = f\"{date_col_name}_temp_ts\"\n",
        "        else:\n",
        "            col_to_use_for_extraction = date_col_name\n",
        "\n",
        "        df_output = df_input\n",
        "        # Cria as colunas de particionamento dinamicamente com base em partition_cols_names\n",
        "        for p_col_name in partition_cols_names:\n",
        "            if p_col_name.endswith(\"_year\"):\n",
        "                df_output = df_output.withColumn(p_col_name, year(col(col_to_use_for_extraction)))\n",
        "            elif p_col_name.endswith(\"_month\"):\n",
        "                df_output = df_output.withColumn(p_col_name, month(col(col_to_use_for_extraction)))\n",
        "\n",
        "        # Opcional: drop a coluna temporária de Timestamp se não for necessária no RAW Parquet\n",
        "        if f\"{date_col_name}_temp_ts\" in df_output.columns:\n",
        "            df_output = df_output.drop(f\"{date_col_name}_temp_ts\")\n",
        "\n",
        "        return df_output\n",
        "    else:\n",
        "        print(f\"Date column '{date_col_name}' not found for preprocessing for partitioning.\")\n",
        "        return df_input\n",
        "\n",
        "def extract_data(archive_path, extract_dir, expected_extensions=['.csv', '.json']):\n",
        "    print(f\"Extracting '{os.path.basename(archive_path)}' to '{extract_dir}'...\")\n",
        "    try:\n",
        "        found_file_path = None\n",
        "\n",
        "        # Primeiro, verificar se um arquivo de dados real já foi extraído\n",
        "        # e não é um arquivo de metadados como '._ab_test_ref.csv'\n",
        "        # Isso ainda é uma heurística, mas mais direcionada\n",
        "        for ext in expected_extensions:\n",
        "            # Tentar o nome do arquivo \"limpo\" derivado do tarball\n",
        "            base_name_without_tar_gz = os.path.basename(archive_path).replace('.tar.gz', '')\n",
        "            potential_extracted_path = os.path.join(extract_dir, base_name_without_tar_gz + ext)\n",
        "            if os.path.exists(potential_extracted_path) and os.path.getsize(potential_extracted_path) > 0:\n",
        "                print(f\"File '{os.path.basename(potential_extracted_path)}' already extracted and is valid. Skipping extraction.\")\n",
        "                return potential_extracted_path\n",
        "\n",
        "        # Se não encontrou arquivo extraído existente e válido, procede com a descompactação\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "\n",
        "        with tarfile.open(archive_path, \"r:gz\") as tar:\n",
        "            # Usamos uma lista para armazenar os membros válidos para processar em ordem\n",
        "            valid_members = []\n",
        "            for member in tar.getmembers():\n",
        "                # Filtrar arquivos de metadados/ocultos e diretórios\n",
        "                # member.isfile() garante que é um arquivo, não um diretório.\n",
        "                # not member.name.startswith('._') e not member.name.startswith('__MACOSX/')\n",
        "                # ignora arquivos ocultos do macOS ou pastas de metadados.\n",
        "                if member.isfile() and \\\n",
        "                   not member.name.startswith('._') and \\\n",
        "                   not member.name.startswith('__MACOSX/') and \\\n",
        "                   any(member.name.lower().endswith(ext) for ext in expected_extensions):\n",
        "                    valid_members.append(member)\n",
        "\n",
        "            # Priorizar arquivos com nomes \"limpos\" (sem subdiretórios inesperados)\n",
        "            # ou o arquivo que parece ser o principal\n",
        "            valid_members.sort(key=lambda m: (\n",
        "                1 if os.path.basename(m.name) == os.path.basename(archive_path).replace('.tar.gz', '.csv') else\n",
        "                2 if os.path.basename(m.name) == os.path.basename(archive_path).replace('.tar.gz', '.json') else\n",
        "                0 # Outros arquivos\n",
        "            ))\n",
        "\n",
        "            for member in valid_members:\n",
        "                print(f\"Extracting '{member.name}'...\")\n",
        "                tar.extract(member, path=extract_dir) # Extrai para o diretório alvo\n",
        "                extracted_path = os.path.join(extract_dir, member.name)\n",
        "\n",
        "                if os.path.exists(extracted_path) and os.path.getsize(extracted_path) > 0:\n",
        "                    found_file_path = extracted_path\n",
        "                    print(f\"File '{member.name}' extracted to '{extracted_path}'.\")\n",
        "                    # Optamos por parar no primeiro arquivo de dados válido encontrado e extraído\n",
        "                    break\n",
        "\n",
        "            if found_file_path:\n",
        "                print(f\"Extract completed. File found: {found_file_path}\")\n",
        "                return found_file_path\n",
        "            else:\n",
        "                print(f\"No valid data files with extensions {expected_extensions} found inside the compressed file, ignoring metadata/hidden files.\")\n",
        "                return None\n",
        "\n",
        "    except tarfile.ReadError:\n",
        "        print(f\"Error: The file '{archive_path}' is not a valid tar.gz archive or is corrupt.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error unpacking '{archive_path}': {e}\")\n",
        "        return None\n",
        "\n",
        "def raw_data_info(raw_data):\n",
        "    numeric_cols = [f.name for f in raw_data.schema\n",
        "                    if isinstance(f.dataType, (DoubleType, LongType, IntegerType))]\n",
        "\n",
        "    print(f\"Total rows: {raw_data.count()}\")\n",
        "    print(\"\\nNull values per column:\")\n",
        "    raw_data.select([count(when(col(c).isNull(), c)).alias(c) for c in raw_data.columns]).show()\n",
        "\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(\"\\nSummary - numeric columns:\")\n",
        "        raw_data.select(*numeric_cols).describe().show()\n",
        "\n",
        "def cast_to_timestamp(df, columns):\n",
        "    for col_name in columns:\n",
        "        if col_name in df.columns and df.schema[col_name].dataType not in (TimestampType(), DateType()):\n",
        "            df = df.withColumn(col_name, to_timestamp(col(col_name)))\n",
        "    return df\n",
        "\n",
        "def cast_to_double(df, columns):\n",
        "    for col_name in columns:\n",
        "        if col_name in df.columns and df.schema[col_name].dataType != DoubleType():\n",
        "            df = df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
        "    return df\n",
        "\n",
        "def cast_to_long(df, columns):\n",
        "    for col_name in columns:\n",
        "        if col_name in df.columns and df.schema[col_name].dataType != LongType():\n",
        "            df = df.withColumn(col_name, col(col_name).cast(LongType()))\n",
        "    return df"
      ],
      "metadata": {
        "id": "w-LlfYdqeMck"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LcGxiL3aFQR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Session initialization"
      ],
      "metadata": {
        "id": "I2JyAsR3XVWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CouponABTest\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "    # .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "\n",
        "\n",
        "print(\"Spark Session initialized!\")"
      ],
      "metadata": {
        "id": "2_s2_ZaJXTwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046a1645-eeba-4e93-cabc-ac6adb6b575e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and inspect raw data"
      ],
      "metadata": {
        "id": "E5mZAwQ_V6I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/order.json.gz\"\n",
        "consumer_url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/consumer.csv.gz\"\n",
        "restaurant_url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/restaurant.csv.gz\"\n",
        "ab_test_ref_url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/ab_test_ref.tar.gz\""
      ],
      "metadata": {
        "id": "kLG0aZD6Yso6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Make download dir"
      ],
      "metadata": {
        "id": "Of7zc3SuZSLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_dir = \"./download_dir\"\n",
        "if not os.path.exists(\"./download_dir\"):\n",
        "    os.makedirs(\"./download_dir\")\n",
        "\n",
        "raw_parquet_dir = \"./raw_parquet\"\n",
        "if not os.path.exists(\"./raw_parquet\"):\n",
        "    os.makedirs(\"./raw_parquet\")"
      ],
      "metadata": {
        "id": "Qcxtpzd4ZYGz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### orders"
      ],
      "metadata": {
        "id": "6gaqrmaehgWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_orders_path = os.path.join(download_dir, \"order.json.gz\")\n",
        "orders_parquet_path = os.path.join(raw_parquet_dir, \"orders.parquet\")\n",
        "\n",
        "if download_data(order_url, local_orders_path):\n",
        "    orders_df_raw = read_data(local_orders_path,\n",
        "                          spark.read.json,\n",
        "                          orders_parquet_path,\n",
        "                          [\"order_created_at\"])\n",
        "else:\n",
        "    orders_df_raw = None"
      ],
      "metadata": {
        "id": "eV36WmlxV9H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_info(orders_df_raw)"
      ],
      "metadata": {
        "id": "jC8r8fo35yZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### consumer"
      ],
      "metadata": {
        "id": "p4lTIw5Nhn8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_consumers_path = os.path.join(download_dir, \"consumer.csv.gz\")\n",
        "consumers_parquet_path = os.path.join(raw_parquet_dir, \"consumers.parquet\")\n",
        "\n",
        "if download_data(consumer_url, local_consumers_path):\n",
        "    consumers_df_raw = read_data(local_consumers_path,\n",
        "                             spark.read.csv,\n",
        "                             consumers_parquet_path,\n",
        "                             header=True)\n",
        "else:\n",
        "    consumers_df_raw = None"
      ],
      "metadata": {
        "id": "5-395HrIdZ15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa410cb9-6cde-4f0d-9650-a0c6f1a7b4db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'consumer.csv.gz' already exists.\n",
            "Reading 'consumer_csv_gz_df' from raw source: ./download_dir/consumer.csv.gz\n",
            "Initial load of 'consumer_csv_gz_df' from raw source completed.\n",
            "Saving 'consumer_csv_gz_df' to raw Parquet: ./raw_parquet/consumers.parquet\n",
            "Successfully saved 'consumer_csv_gz_df' to raw Parquet.\n",
            "\n",
            "DataFrame 'consumer_csv_gz_df' from Parquet (raw):\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- language: string (nullable = true)\n",
            " |-- created_at: string (nullable = true)\n",
            " |-- active: string (nullable = true)\n",
            " |-- customer_name: string (nullable = true)\n",
            " |-- customer_phone_area: string (nullable = true)\n",
            " |-- customer_phone_number: string (nullable = true)\n",
            "\n",
            "+--------------------+--------+--------------------+------+-------------+-------------------+---------------------+\n",
            "|         customer_id|language|          created_at|active|customer_name|customer_phone_area|customer_phone_number|\n",
            "+--------------------+--------+--------------------+------+-------------+-------------------+---------------------+\n",
            "|e8cc60860e09c0bb1...|   pt-br|2018-04-05T14:49:...|  true|         NUNO|                 46|            816135924|\n",
            "|a2834a38a9876cf74...|   pt-br|2018-01-14T21:40:...|  true|     ADRIELLY|                 59|            231330577|\n",
            "|41e1051728eba1334...|   pt-br|2018-01-07T03:47:...|  true|        PAULA|                 62|            347597883|\n",
            "|8e7c1dcb64edf95c9...|   pt-br|2018-01-10T22:17:...|  true|       HELTON|                 13|            719366842|\n",
            "|7823d4cf4150c5dae...|   pt-br|2018-04-06T00:16:...|  true|       WENDER|                 76|            543232158|\n",
            "+--------------------+--------+--------------------+------+-------------+-------------------+---------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_info(consumers_df_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYgBJpX4ZdeM",
        "outputId": "d6153eac-ccab-480c-bef7-922b3269b011"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 806156\n",
            "\n",
            "Null values per column:\n",
            "+-----------+--------+----------+------+-------------+-------------------+---------------------+\n",
            "|customer_id|language|created_at|active|customer_name|customer_phone_area|customer_phone_number|\n",
            "+-----------+--------+----------+------+-------------+-------------------+---------------------+\n",
            "|          0|       0|         0|     0|            0|                  0|                    0|\n",
            "+-----------+--------+----------+------+-------------+-------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### restaurant"
      ],
      "metadata": {
        "id": "k02clXsYh9SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_restaurants_path = os.path.join(download_dir, \"restaurant.csv.gz\")\n",
        "restaurants_parquet_path = os.path.join(raw_parquet_dir, \"restaurants.parquet\")\n",
        "\n",
        "if download_data(restaurant_url, local_restaurants_path):\n",
        "    restaurants_df_raw = read_data(local_restaurants_path,\n",
        "                               spark.read.csv,\n",
        "                               restaurants_parquet_path,\n",
        "                               header=True)\n",
        "else:\n",
        "    restaurants_df_raw = None"
      ],
      "metadata": {
        "id": "eLwhSoEyh3H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c559477d-c005-49a9-9434-1c1809fb6ea2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'restaurant.csv.gz' already exists.\n",
            "Reading 'restaurant_csv_gz_df' from raw source: ./download_dir/restaurant.csv.gz\n",
            "Initial load of 'restaurant_csv_gz_df' from raw source completed.\n",
            "Saving 'restaurant_csv_gz_df' to raw Parquet: ./raw_parquet/restaurants.parquet\n",
            "Successfully saved 'restaurant_csv_gz_df' to raw Parquet.\n",
            "\n",
            "DataFrame 'restaurant_csv_gz_df' from Parquet (raw):\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- created_at: string (nullable = true)\n",
            " |-- enabled: string (nullable = true)\n",
            " |-- price_range: string (nullable = true)\n",
            " |-- average_ticket: string (nullable = true)\n",
            " |-- takeout_time: string (nullable = true)\n",
            " |-- delivery_time: string (nullable = true)\n",
            " |-- minimum_order_value: string (nullable = true)\n",
            " |-- merchant_zip_code: string (nullable = true)\n",
            " |-- merchant_city: string (nullable = true)\n",
            " |-- merchant_state: string (nullable = true)\n",
            " |-- merchant_country: string (nullable = true)\n",
            "\n",
            "+--------------------+--------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "|                  id|          created_at|enabled|price_range|average_ticket|takeout_time|delivery_time|minimum_order_value|merchant_zip_code| merchant_city|merchant_state|merchant_country|\n",
            "+--------------------+--------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "|d19ff6fca6288939b...|2017-01-23T12:52:...|  false|          3|          60.0|           0|           50|               30.0|            14025|RIBEIRAO PRETO|            SP|              BR|\n",
            "|631df0985fdbbaf27...|2017-01-20T13:14:...|   true|          3|          60.0|           0|            0|               30.0|            50180|     SAO PAULO|            SP|              BR|\n",
            "|135c5c4ae4c1ec1fd...|2017-01-23T12:46:...|   true|          5|         100.0|           0|           45|               10.0|            23090|RIO DE JANEIRO|            RJ|              BR|\n",
            "|d26f84c470451f752...|2017-01-20T13:15:...|   true|          3|          80.0|           0|            0|               18.9|            40255|      SALVADOR|            BA|              BR|\n",
            "|97b9884600ea71923...|2017-01-20T13:14:...|   true|          3|          60.0|           0|            0|               25.0|            64600|       BARUERI|            SP|              BR|\n",
            "+--------------------+--------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_info(restaurants_df_raw)"
      ],
      "metadata": {
        "id": "AV88Wap3UGPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801bc29d-a450-4e3d-c540-c0df4d93d309"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 7292\n",
            "\n",
            "Null values per column:\n",
            "+---+----------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+-------------+--------------+----------------+\n",
            "| id|created_at|enabled|price_range|average_ticket|takeout_time|delivery_time|minimum_order_value|merchant_zip_code|merchant_city|merchant_state|merchant_country|\n",
            "+---+----------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+-------------+--------------+----------------+\n",
            "|  0|         0|      0|          0|             0|           0|            1|                 95|                0|            0|             0|               0|\n",
            "+---+----------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+-------------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ab_test_ref"
      ],
      "metadata": {
        "id": "7R7ZWWLeiyXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_ab_test_tar_path = os.path.join(download_dir, \"ab_test_ref.tar.gz\")\n",
        "local_ab_test_path = os.path.join(download_dir, \"ab_test_ref.csv\")\n",
        "ab_test_ref_parquet_path = os.path.join(raw_parquet_dir, \"ab_test_ref.parquet\")\n",
        "ab_test_df_raw = None\n",
        "\n",
        "if download_data(ab_test_ref_url, local_ab_test_tar_path):\n",
        "    expected_ab_test_extensions = ['.csv', '.json']\n",
        "    extracted_ab_test_file = extract_data(\n",
        "        local_ab_test_tar_path,\n",
        "        download_dir,\n",
        "        expected_extensions=expected_ab_test_extensions\n",
        "    )\n",
        "\n",
        "    if extracted_ab_test_file:\n",
        "        if extracted_ab_test_file.endswith('.csv'):\n",
        "            ab_test_df_raw = read_data(extracted_ab_test_file,\n",
        "                                   spark.read.csv,\n",
        "                                   ab_test_ref_parquet_path,\n",
        "                                   header=True,\n",
        "                                   inferSchema=True)\n",
        "        elif extracted_ab_test_file.endswith('.json'):\n",
        "            ab_test_df_raw = read_data(extracted_ab_test_file,\n",
        "                                   spark.read.json,\n",
        "                                   ab_test_ref_parquet_path)\n",
        "        else:\n",
        "            print(f\"File format '{extracted_ab_test_file}' not supported for direct Spark loading.\")\n",
        "    else:\n",
        "        print(\"Could not find a compatible data file in the A/B archive.\")\n",
        "else:\n",
        "    print(\"ab_test_ref.tar.gz file download failed.\")"
      ],
      "metadata": {
        "id": "xQEkiiAgixI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49e78d7-8181-441e-bda5-f75d91d02c42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'ab_test_ref.tar.gz' already exists.\n",
            "Extracting 'ab_test_ref.tar.gz' to './download_dir'...\n",
            "File 'ab_test_ref.csv' already extracted and is valid. Skipping extraction.\n",
            "Reading 'ab_test_ref_csv_df' from raw source: ./download_dir/ab_test_ref.csv\n",
            "Initial load of 'ab_test_ref_csv_df' from raw source completed.\n",
            "Saving 'ab_test_ref_csv_df' to raw Parquet: ./raw_parquet/ab_test_ref.parquet\n",
            "Successfully saved 'ab_test_ref_csv_df' to raw Parquet.\n",
            "\n",
            "DataFrame 'ab_test_ref_csv_df' from Parquet (raw):\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- is_target: string (nullable = true)\n",
            "\n",
            "+--------------------+---------+\n",
            "|         customer_id|is_target|\n",
            "+--------------------+---------+\n",
            "|755e1fa18f25caec5...|   target|\n",
            "|b821aa8372b8e5b82...|  control|\n",
            "|d425d6ee4c9d4e211...|  control|\n",
            "|6a7089eea0a5dc294...|   target|\n",
            "|dad6b7e222bab31c0...|  control|\n",
            "+--------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_info(ab_test_df_raw)"
      ],
      "metadata": {
        "id": "jVBHy7dyuvfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e7dc5e-32f3-47ac-e3f5-9b94a020d0eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 806467\n",
            "\n",
            "Null values per column:\n",
            "+-----------+---------+\n",
            "|customer_id|is_target|\n",
            "+-----------+---------+\n",
            "|          0|        0|\n",
            "+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning and Enriching"
      ],
      "metadata": {
        "id": "kd2yQf9Ai8DA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "l8wHHpSWjN6F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WB2A_kNSjNmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### orders"
      ],
      "metadata": {
        "id": "-r_S8cFIjIZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = orders_df_raw\n",
        "\n",
        "cols_timestamp = [\"order_created_at\", \"order_scheduled_date\"]\n",
        "cols_double = [\"delivery_address_latitude\",\n",
        "               \"delivery_address_longitude\",\n",
        "               \"merchant_latitude\",\n",
        "               \"merchant_longitude\"]\n",
        "\n",
        "orders_df = cast_to_timestamp(orders_df, cols_timestamp)\n",
        "orders_df = cast_to_double(orders_df, cols_double)\n",
        "\n",
        "orders_df.printSchema()"
      ],
      "metadata": {
        "id": "d4aFf1xijBYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.select(\"items\") \\\n",
        ".where(\"customer_name == 'GUSTAVO' and order_created_at < '2019-01-17'\").collect()[3]"
      ],
      "metadata": {
        "id": "1TFCl5iAlvL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCPcKdhnnp8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}